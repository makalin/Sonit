# Sonit

**Translating the Unspoken.**

Sonit is an open-source translator for non-verbal vocal gestures such as murmurs, hums, and culturally meaningful sounds (e.g., â€œhÄ±hâ€, â€œÄ±hÄ±â€, â€œtskâ€). It aims to give a voice to those who cannot speak, by interpreting subtle audio cues and translating them into words, intentions, or actions.

---

## ğŸ§  Overview

Sonit bridges the gap between vocal expression and spoken language. Itâ€™s designed for individuals with aphonia, neurological conditions, or temporary vocal loss, and for researchers working on humanâ€“machine interaction with minimal audio signals.

Sonit learns how a user expresses meaning through sound â€” then builds a personalized model to translate those expressions.

---

## ğŸ”§ Tech Stack

- **Python** â€” Core logic, training pipeline
- **Kivy** â€” Lightweight cross-platform GUI (mobile + desktop)
- **PyTorch** â€” Deep learning model for sound classification
- **NumPy, Librosa** â€” Audio signal processing
- **SQLite** â€” Local user-specific training data

---

## ğŸ” Features

- ğŸ™ï¸ **Sound Input** â€” Captures vocal gestures like â€œmurâ€, â€œtskâ€, â€œuhhâ€
- ğŸ§¬ **Sound-to-Intent Model** â€” Learns how each user expresses approval, refusal, interest, etc.
- ğŸ§  **Training Mode** â€” User/caregiver can label sounds and build a unique translation set
- ğŸ§¾ **Live Translation** â€” Real-time feedback showing interpreted meaning
- ğŸ“Š **Model Viewer** â€” See sound embeddings, confidence levels, and live output

---

## ğŸ’¡ Use Cases

- Patients with vocal impairments (ALS, trauma, surgery recovery)
- Non-verbal communication training
- Assistive technology prototypes
- Research in minimal-signal communication

---

## ğŸ“ Repository Structure

```

Sonit/
â”œâ”€â”€ app/                # Kivy app UI
â”œâ”€â”€ model/              # AI models & training
â”œâ”€â”€ audio/              # Input, recording, analysis
â”œâ”€â”€ data/               # User datasets
â”œâ”€â”€ utils/              # Helper functions
â”œâ”€â”€ main.py             # App entry point
â””â”€â”€ README.md

````

---

## ğŸš€ Getting Started

```bash
git clone https://github.com/makalin/Sonit.git
cd Sonit
pip install -r requirements.txt
python main.py
````

**Dependencies**: Python 3.9+, PyTorch, Kivy, Librosa, NumPy, SoundFile

---

## ğŸ“ˆ Roadmap

* [x] Real-time sound capture
* [x] Labeling & training with minimal sounds
* [ ] Context-based intent prediction
* [ ] Model export/import across devices
* [ ] Caregiver override and suggestions
* [ ] Turkish sound gesture model (e.g., "Ä±hÄ±", "hÄ±h", "hÄ±mm")

---

## ğŸ§ª Research

We're developing a small open dataset of Turkish vocal gestures. If you're a linguist, clinical expert, or accessibility researcher, get in touch!

---

## ğŸ“„ License

MIT License â€” free for personal, academic, and commercial use.

---

## ğŸ™‹â€â™€ï¸ Contributing

1. Fork the repo
2. Run the dev version and test with your own sounds
3. Submit improvements, bugfixes, or models via PR

We welcome help from:

* Accessibility advocates
* Audio researchers
* Speech therapists
* UI/UX designers for assistive apps

---

## ğŸ“¬ Contact

> GitHub: [github.com/makalin/sonit](https://github.com/makalin/sonit)
> Email: [makalin@gmail.com](mailto:makalin@gmail.com)

---

> *â€œNot every voice speaks in words. Sonit listens anyway.â€*
